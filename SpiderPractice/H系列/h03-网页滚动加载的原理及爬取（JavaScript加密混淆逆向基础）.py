# _*_ coding: utf-8 _*_
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2023å¹´12æœˆ30
@ ğŸ“„File       : h03-ç½‘é¡µæ»šåŠ¨åŠ è½½çš„åŸç†åŠçˆ¬å–ï¼ˆJavaScriptåŠ å¯†æ··æ·†é€†å‘åŸºç¡€ï¼‰.py
@ â„¹ï¸Description:
ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/h03/
è¿™ä¸ªç½‘é¡µæ˜¯æœ‰é—®é¢˜çš„ï¼Œåœ¨ä¸‹æ‹‰åå¹¶æ²¡æœ‰åƒé¢„æ–™çš„é‚£æ ·æ›´æ–°æ–°å†…å®¹ã€‚

-- ç¬¬ä¸€é¡µçš„æœ€åä¸€ä¸ªæ˜¯æ³°å¦å°¼å…‹å·ã€‚
5f685274073b -- 2
279fcd874c72 -- 3
8a3d4d640516 -- 4

http://www.spiderbuf.cn/h03/5f685274073b æ¥å£é“¾æ¥å½¢å¼ï¼Œç¡®å®šæ¥å£æ˜¯ç¬¬å‡ é¡µå¯ä»¥å¤åˆ¶ /5f685274073b åˆ°XHRæ–­ç‚¹è°ƒè¯•çŸ¥æ™“

å…¶å®å°±æ˜¯ajaxåŠ¨æ€åŠ è½½æ•°æ®çš„çˆ¬å–ã€‚å…¶å®ä¹Ÿä¸éš¾ï¼Œç›´æ¥æ‰“å¼€æ§åˆ¶å°æŸ¥çœ‹XHRçš„æ¥å£ä¿¡æ¯ï¼Œå®šä½åˆ°æ•°æ®æ¥å£ï¼Œåˆ†æå®ƒçš„æœ‰æ²¡æœ‰æºå¸¦æ§åˆ¶é¡µæ•°çš„å‚æ•°ï¼ˆè½½è·ï¼‰
ç„¶ååˆ¤æ–­ï¼Œå“ªäº›æ˜¯å›ºå®šå€¼ï¼Œå“ªäº›æ˜¯åŠ¨æ€å€¼ï¼Œé€†å‘å‡ºåŠ¨æ€å€¼åæ„é€ å‡ºè¯·æ±‚æ—¶è¦æºå¸¦çš„è¯·æ±‚å‚æ•°å³å¯ã€‚è¿™æ˜¯é€šç”¨çš„é€»è¾‘ï¼Œä½†å…·ä½“è¿˜å¾—çœ‹å®é™…æƒ…å†µã€‚

â€œä»£ç ç»è¿‡GPTä¼˜åŒ–åï¼Œå…·å¤‡äº†è‰¯å¥½çš„å¯è¯»æ€§ã€å¥å£®æ€§å’Œæ‰©å±•æ€§ï¼Œé™ä½äº†å­¦ä¹ éš¾åº¦ï¼Œæé«˜äº†ç¼–ç æ•ˆç‡å’Œç¼–ç¨‹æ€ç»´ã€‚â€
"""
import os
import time
from typing import Generator, Optional, List, Dict
from urllib.parse import urljoin

import requests
from lxml import etree

FOLDER_NAME = 'datas'
os.makedirs(FOLDER_NAME, exist_ok=True)


def get_page(url: str, headers: Optional[dict] = None, retries: int = 3) -> Optional[str]:
    """
    é€šç”¨è¯·æ±‚æ–¹æ³•ï¼Œæ”¯æŒé‡è¯•
    :param url: è¯·æ±‚çš„URL
    :param headers: è¯·æ±‚å¤´
    :param retries: é‡è¯•æ¬¡æ•°
    :return: å“åº”æ–‡æœ¬æˆ–None
    """
    default_headers = headers or {
        'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                       '(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
    }

    for attempt in range(retries):
        try:
            response = requests.get(url, headers=default_headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Error making request to {url}: {e}. Attempt {attempt + 1} of {retries}.")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿ç­–ç•¥
    return None


def get_api_url(url: str, html: Optional[str] = None) -> Generator[str, None, None]:
    """
    ç”Ÿæˆä¸‹ä¸€é¡µçš„URL
    :param url: åŸºç¡€URL
    :param html: ç½‘é¡µæºç 
    :return: ä¸‹ä¸€é¡µçš„URLç”Ÿæˆå™¨
    """
    if html is None:
        html = get_page(url)
        if not html:
            return

    dom_tree = etree.HTML(html)
    while html:
        next_paths = dom_tree.xpath('//div[@id="sLaOuol2SM0iFj4d"]/text()')
        if next_paths:
            next_path = next_paths[0]
            next_page_url = urljoin(url, next_path)
            yield next_page_url
            html = get_page(next_page_url)
            if html:
                dom_tree = etree.HTML(html)
            else:
                break
        else:
            print('No next page found.')
            break


def parse_page(html: str) -> List[Dict[str, str]]:
    """
    è§£æç½‘é¡µå†…å®¹
    :param html: ç½‘é¡µæºç 
    :return: è§£æåçš„æ•°æ®åˆ—è¡¨
    """
    dom = etree.HTML(html)
    div_list = dom.xpath('//div[contains(@class, "row") and contains(@style, "margin-top: 10px;")]/div')[::2]  # éš”è¡Œå–å€¼ï¼Œç­›é€‰æ‰ç®€ä»‹
    parsed_data = []
    for div in div_list:
        title = div.xpath('./h2/text()')[0]
        scores = div.xpath('./div[2]/span[contains(text(),"è±†ç“£ç”µå½±è¯„åˆ†:")]/following::text()[1]')[0]
        parsed_data.append({'title': title, 'scores': scores})
    return parsed_data


def save_data(data: List[Dict[str, str]], file_name: str = 'h03ç”µå½±ä¿¡æ¯.csv') -> None:
    """
    æ•°æ®ä¿å­˜
    :param data: éœ€è¦ä¿å­˜çš„æ•°æ®
    :param file_name: æ–‡ä»¶å
    """
    path = os.path.join(FOLDER_NAME, file_name)
    with open(path, 'a', encoding='utf-8') as f:
        for item in data:
            f.write(f"{item['title']},{item['scores']}\n")
        print('ä¿å­˜å®Œæ¯•')


def main() -> None:
    base_url = 'http://www.spiderbuf.cn/h03/'
    # æ¥å£é“¾æ¥çš„å½¢å¼æ˜¯ä»ç¬¬äºŒé¡µå¼€å§‹çš„ï¼Œç¬¬ä¸€é¡µæˆ‘ä»¬éœ€å•ç‹¬å¤„ç†
    index_page_html = get_page(base_url)
    if index_page_html:
        index_page_data = parse_page(index_page_html)
        save_data(index_page_data)

        gen_next_url = get_api_url(base_url, index_page_html)
        for next_url in gen_next_url:
            html_data = get_page(next_url)
            if html_data:
                parsed_data = parse_page(html_data)
                save_data(parsed_data)


if __name__ == '__main__':
    main()

