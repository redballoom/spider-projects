# _*_ coding: utf-8 _*_
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2023å¹´12æœˆ30
@ ğŸ“„File       : h01-CSSæ ·å¼åç§»æ··æ·†æ–‡æœ¬å†…å®¹çš„è§£æä¸çˆ¬å–.py
@ â„¹ï¸Description:
ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/h01/

å¤„ç†csså­—ç¬¦åç§»çš„åçˆ¬æ¡ˆä¾‹
"""

# å¯¼å…¥éœ€è¦çš„åº“
import os
import requests
import csv
from lxml import etree


def get_page(url, headers=None, retries=3):
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    }

    for _ in range(retries):
        try:
            response = requests.get(url, headers=default_headers, timeout=10)
            response.raise_for_status()  # å¦‚æœä¸æ˜¯200çŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
            return response.text
        except requests.exceptions.RequestException as e:  # é€šç”¨å¼‚å¸¸æ•è·
            print(f"Error making request to {url}: {e}")
    return None


def lxml_parse(html):
    """
    ä½¿ç”¨ lxml çš„ etree è§£æ HTML
    :param html: ä¸€é¡µçš„ç½‘é¡µæºä»£ç æ•°æ®
    :return page_data_list: è§£æåçš„ä¸€é¡µå­—å…¸æ•°æ®åˆ—è¡¨
    """

    # <div class="col-xs-6 col-lg-4" style="margin-bottom: 30px;">
    #         <h2><i style="width: 32px;position: relative; left: 32px;">èŠ‚</i><i style="width: 32px;position: relative; left: -32px;">å­—</i><i style="width: 32px;position: relative;">è·³</i><i style="width: 32px;position: relative;">åŠ¨</i></h2>
    #         <p>æ’åï¼š4</p>
    #         <p>ä¼ä¸šä¼°å€¼(äº¿å…ƒ)ï¼š<i style="width: 14px;position: relative; left: 10px;">2</i><i style="width: 14px;position: relative; left: -10px;">2</i><i style="width: 14px;position: relative;">5</i><i style="width: 14px;position: relative;">0</i></p>
    #         <p>CEOï¼šå¼ åˆ©ä¸œ</p>
    #         <p>è¡Œä¸šï¼šä¼ åª’å’Œå¨±ä¹</p>
    #     </div><!--/.col-xs-6.col-lg-4-->
    # æ‰¾åˆ°å¯¹åº”å­—ç¬¦çš„åç§»ï¼ˆäº¤æ¢ï¼‰çš„è§„å¾‹ï¼Œåœ¨pythonä¸­è§£æå‡ºå­—ç¬¦ä¸²åäº¤æ¢ä½ç½®å³å¯
    page_data_list = []
    dom_tree = etree.HTML(html)
    divs = dom_tree.xpath('//div[@class="container"]/div/div')
    for div in divs:
        title = div.xpath('./h2//text()')
        title[0], title[1] = title[1], title[0]  # äº¤æ¢ä¸‹æ ‡0å’Œ1çš„å€¼, å…¶å®å°±æ˜¯äº¤æ¢åç§»çš„å€¼
        company_name = ''.join(title)

        ranking = div.xpath('./p[1]/text()')[0].replace('æ’åï¼š', '')

        value = div.xpath('./p[2]//text()')[1:]
        value[0], value[1] = value[1], value[0]
        company_value = ''.join(value)

        ceo_name = div.xpath('./p[3]/text()')[0].replace('CEOï¼š', '')
        profession = div.xpath('./p[4]/text()')[0].replace('è¡Œä¸šï¼š', '')
        dic = {'ä¼ä¸š': company_name,
               'æ’å': ranking,
               'ä¼ä¸šä¼°å€¼(äº¿å…ƒ)': company_value,
               'CEO': ceo_name,
               'è¡Œä¸š': profession, }
        page_data_list.append(dic)
    return page_data_list


def save_data(datas, file_name='h01èƒ¡æ¶¦ä¸­å›½500å¼º.csv'):
    """æ•°æ®ä¿å­˜"""
    save_directory = 'datas'
    os.makedirs(save_directory, exist_ok=True)  # exist_ok=True å¦‚æœç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™ä¸å¼•å‘é”™è¯¯, Falseåˆ™åä¹‹

    csv_header = ['ä¼ä¸š', 'æ’å', 'ä¼ä¸šä¼°å€¼(äº¿å…ƒ)', 'CEO', 'è¡Œä¸š']
    path = os.path.join(save_directory, file_name)
    write_header = not os.path.exists(path)  # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬æ‰å†™å…¥csv_header
    with open(path, mode='a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=csv_header)
        if write_header:  # æ ¹æ® write_header çš„åˆ¤æ–­ï¼Œç¡®ä¿åªä¼šå†™ä¸€æ¬¡å¤´ä¿¡æ¯
            writer.writeheader()
        writer.writerows(datas)


def main():
    url = 'http://www.spiderbuf.cn/h01/'
    html_data = get_page(url)
    if html_data:
        data_list = lxml_parse(html_data)
        save_data(data_list)
    print('All Done!')


if __name__ == '__main__':
    main()
