# -*- coding:utf-8 -*-
"""
âœï¸Author     : ğŸˆ
â²ï¸Time       : 2024/5/16
ğŸ“„File       : 52pj.py
â„¹ï¸Description: é‡‡é›†52pjç½‘çš„æ·˜è´´çš„æ‰€æœ‰ä¸“è¾‘æ•°æ®ä¿¡æ¯å¹¶ä¿å­˜è‡³csvæ–‡ä»¶
æ­¤ä»£ç ä¸­å¤§é‡é‡‡ç”¨GPTç»™å‡ºçš„ä¼˜åŒ–å»ºè®®ï¼Œæ‰€ä»¥ä½ ä¼šçœ‹åˆ°å¾ˆå¤šæ²¡ä½¿ç”¨è¿‡çš„æ–¹æ³•ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼Œå­¦ä¹ å°±å®Œäº†ã€‚
"""

import random
import time
import re
import types
from urllib.parse import urljoin
import csv
import urllib3

from loguru import logger
import requests
from lxml import etree
from requests.utils import get_encoding_from_headers

base_url = 'https://www.52pojie.cn/'

# æå‰å†™å…¥å¤´éƒ¨ä¿¡æ¯ï¼Œé¿å…åœ¨åç»­è°ƒç”¨æ—¶å¤šæ¬¡å†™å…¥
fieldnames = ['album_title', 'album_url', 'album_rss', 'album_review', 'album_author', 'album_latest_time']
with open('data1.csv', 'w', newline='', encoding='utf-8') as w_file:
    writer_header = csv.DictWriter(w_file, fieldnames=fieldnames)
    writer_header.writeheader()

# ç¦æ­¢æ˜¾ç¤ºInsecureRequestWarningè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def get_page(url):
    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Connection": "keep-alive",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    }

    try:
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        # ä½¿ç”¨ response.apparent_encoding å¯èƒ½æ›´ä¿é™©ï¼Œå½“æ²¡æœ‰æ˜ç¡®charsetæ—¶
        response.encoding = (
            requests.utils.get_encoding_from_headers(response.headers)
            if 'charset' in response.headers.get('content-type', '').lower()
            else response.apparent_encoding
        )
        return response.text
    except requests.exceptions.Timeout as e:
        logger.exception(f"è¯·æ±‚è¶…æ—¶é”™è¯¯ {url}: {e}")
        raise  # å‘ä¸Šå±‚æŠ›å‡ºå¼‚å¸¸ï¼Œä¸åœ¨è¿™é‡Œå¤„ç†
    except requests.exceptions.RequestException as e:
        logger.exception(f"è¯·æ±‚é”™è¯¯ {url}: {e}")
        raise  # å‘ä¸Šå±‚æŠ›å‡ºå¼‚å¸¸


def get_index_page(page):
    """
    è·å–52pojie.cnè®ºå›æ”¶è—é¡µçš„HTMLæ•°æ®å¹¶è¿›è¡Œè§£æã€‚

    å‚æ•°:
    page (int): éœ€è¦æŠ“å–çš„æ”¶è—é¡µæ•°

    è¿”å›:
    - å¦‚æœè§£ææˆåŠŸï¼Œè¿”å›è§£æåçš„æ•°æ®ã€‚
    - å¦‚æœè¯·æ±‚å¤±è´¥æˆ–è§£æå¤±è´¥ï¼Œè¿”å›ç›¸åº”çš„é”™è¯¯ä¿¡æ¯ã€‚
    - å¦‚æœåœ¨è·å–æˆ–è§£æè¿‡ç¨‹ä¸­é‡åˆ°å¼‚å¸¸ï¼Œè¿”å›å¼‚å¸¸é”™è¯¯ä¿¡æ¯ã€‚
    """
    page_url = f'https://www.52pojie.cn/forum.php?mod=collection&order=dateline&op=all&page={page}'
    try:
        html_data = get_page(page_url)
        # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        if html_data:
            # éšæœºå»¶è¿Ÿåè¿›è¡Œè§£æ
            cho = [0.5, 0.8, 1, 1.2, 1.5, 1.8, 2, 2.5]
            time.sleep(random.choice(cho))
            logger.info(f'......å¼€å§‹è§£æç¬¬{page}é¡µ......')
            parsed_data = parse(html_data)
            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if parsed_data:
                return parsed_data
            else:
                logger.error("è§£æå¤±è´¥ï¼Œæœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
                return "è§£æé”™è¯¯"
        else:
            logger.error(f"è¯·æ±‚ç¬¬{page}é¡µå¤±è´¥ï¼Œæœªè·å–åˆ°HTMLæ•°æ®")
            return "è¯·æ±‚é”™è¯¯"
    except Exception as e:
        logger.exception(f"åœ¨è·å–æˆ–è§£æç¬¬{page}é¡µæ—¶å‡ºç°é”™è¯¯: {e}")
        return "å¼‚å¸¸é”™è¯¯"


def parse(html):
    """
    è§£æHTMLæ•°æ®ï¼Œæå–ä¸“è¾‘ä¿¡æ¯ã€‚

    å‚æ•°:
    html (str): HTMLæ•°æ®
    base_url (str): åŸºç¡€URLï¼Œç”¨äºæ„å»ºç»å¯¹URL

    è¿”å›:
    - å¦‚æœè§£ææˆåŠŸï¼Œè¿”å›åŒ…å«ä¸“è¾‘ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ã€‚
    - å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯ã€‚
    """
    try:
        tree = etree.HTML(html)
        divs = tree.xpath('//div[contains(@class, "clct_list")]/div')
        for item in divs:
            album_title = item.xpath('./dl/dt/div/a/text()')[0]
            album_path = item.xpath('./dl/dt/div/a/@href')[0]
            album_url = urljoin(base_url, album_path)
            album_info = item.xpath('./dl/dd[2]/p[2]/text()')[0]
            album_rss = re.search(r'è®¢é˜… (\d+)', album_info).group(1) if re.search(r'è®¢é˜… (\d+)', album_info) else ''  # è®¢é˜…
            album_review = re.search(r'è¯„è®º (\d+)', album_info).group(1) if re.search(r'è¯„è®º (\d+)', album_info) else ''  # è¯„è®º
            album_author = item.xpath('./dl/dd[2]/p[3]/a/text()')[0]
            album_update_time = item.xpath('./dl/dd[2]/p[3]/text()')[0]
            album_latest_time = re.sub('åˆ›å»º, æœ€åæ›´æ–°', '', album_update_time)

            dit = {
                'album_title': album_title,  # ä¸“è¾‘æ ‡é¢˜
                'album_url': album_url,  # ä¸“è¾‘é“¾æ¥
                'album_rss': album_rss,  # ä¸“è¾‘è®¢é˜…
                'album_review': album_review,  # ä¸“è¾‘è¯„è®º
                'album_author': album_author,  # ä¸“è¾‘ä½œè€…
                'album_latest_time': album_latest_time,  # ä¸“è¾‘æœ€æ–°æ—¶é—´
            }
            yield dit
    except IndexError as e:
        logger.error(f"è§£æå¤±è´¥ï¼Œæœªè·å–åˆ°æœ‰æ•ˆæ•°æ®: {e}")
        yield "è§£æé”™è¯¯"
    except Exception as e:
        logger.exception(f"åœ¨è§£æHTMLæ—¶å‡ºç°é”™è¯¯: {e}")
        yield "è§£æé”™è¯¯"


def save_to_csv(data):
    """
    å°†æ¥æ”¶çš„æ•°æ®ä¿å­˜ä¸ºcsvæ–‡ä»¶
    :param data: æ¥æ”¶ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡
    :return:
    """
    with open('data.csv', 'a', newline='', encoding='utf-8') as a_file:
        writer = csv.DictWriter(a_file, fieldnames=fieldnames)
        for row in data:
            writer.writerow(row)


def main():
    for i in range(1, 3):
        gen_page_data = get_index_page(i)
        if isinstance(gen_page_data, types.GeneratorType):  # åªè¦æ­£ç¡®è¿”å›å€¼æ‰å¤„ç†
            save_to_csv(gen_page_data)
            logger.info(f'ä¿å­˜ç¬¬{i}é¡µæ•°æ®å®Œæ¯•')


if __name__ == '__main__':
    main()
