# _*_ coding: utf-8 _*_
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2023å¹´12æœˆ31
@ ğŸ“„File       : h04-jsåŠ å¯†æ··æ·†åŠç®€å•åè°ƒè¯•.py
@ â„¹ï¸Description:
ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/h04/

å…¶å®å°±è·Ÿajaxä¸€æ ·ï¼Œæ‰¾åˆ°å¯¹åº”çš„æ•°æ®æ¥å£å³å¯ï¼Œä¸è¿‡è¿™æ˜¯åœ¨jsæ–‡ä»¶ä¸­

æ­¥éª¤ï¼š
    - å¯¹äºdebuggerï¼Œå¯ä»¥å³é”®ä¸€å¾‹ä¸åœ¨æ­¤å±•å…æˆ–è®¾ç½®æ¡ä»¶è¿‡æ‰ï¼Œä½†è¿™ä¹Ÿæ²»æ ‡ä¸æ²»æœ¬ï¼Œä¸€åŠ³æ°¸é€¸çš„æ–¹æ³•æ˜¯HOOK debuggerçš„æ–­ç‚¹ä½ç½®ï¼Œè¿™éœ€è¦è‡ªè¡Œå»å­¦ä¹ ã€‚
    - åˆ·æ–°ç½‘é¡µåçœ‹åˆ°æˆ‘ä»¬çš„docæ–‡ä»¶ä¸­æ²¡æœ‰æˆ‘ä»¬éœ€è¦çš„æ•°æ®ï¼Œæ­¤æ—¶å¯æ–­å®šå®ƒæ˜¯åŠ¨æ€åŠ è½½çš„æ•°æ®ï¼Œå°±è¦å»æ‰¾å…¶æ•°æ®åŠ è½½çš„ä½ç½®æˆ–æ–‡ä»¶ã€æ¥å£
    - ctrl + f ç›´æ¥æœç´¢å¯†ç çš„æ•°æ®ï¼Œèƒ½çœ‹åˆ°ä¸€ä¸ªjsæ–‡ä»¶ï¼Œå…¶ä¸­å°±åŒ…å«æ•°æ®ä¿¡æ¯ï¼Œåªéœ€è¦è·å–ä¸‹æ¥å³å¯ã€‚

å¯¹äºjsæ–‡ä»¶ï¼Œæˆ‘ä»¬å…ˆå‰ä¸€ç›´ä½¿ç”¨çš„xpathè§£æå°±æ— èƒ½ä¸ºåŠ›äº†ï¼Œreåº“å°±å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä»jsä»£ç ä¸­æå–éœ€è¦çš„ä¿¡æ¯ã€‚
"""
import os
import csv
from time import sleep
import re
import json

import requests

FOLDER_NAME = 'datas'
os.makedirs(FOLDER_NAME, exist_ok=True)


def get_page(url, headers=None, retries=3):
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    }

    for attempt in range(retries):
        try:
            response = requests.get(url, headers=default_headers, timeout=10)
            response.raise_for_status()  # å¦‚æœä¸æ˜¯200çŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
            return response.text
        except requests.exceptions.RequestException as e:  # é€šç”¨å¼‚å¸¸æ•è·
            print(f"Error making request to {url}: {e}. Attempt {attempt + 1} of {retries}.")
            sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿ç­–ç•¥
    return None


def json_to_dict(json_str):
    # å…ˆè¿›è¡ŒUnicodeè½¬ä¹‰è§£ç 
    json_data = json_str.encode('utf-8').decode('unicode_escape').replace('\'', '"')
    # å°†åå…­è¿›åˆ¶æ•°æ›¿æ¢ä¸ºåè¿›åˆ¶æ•°
    json_data = re.sub(r'0x[0-9a-fA-F]+', lambda x: str(int(x.group(0), 16)), json_data)
    json_data = json.loads(json_data)  # è½¬æ¢ä¸ºæ ‡å‡†jsonæ ¼å¼ï¼Œé¿å…å‡ºç°é”™è¯¯
    return json_data


def parse_data(js_data):
    pattern = re.compile(r'data=(.*?);', re.S)
    json_data = re.search(pattern, js_data).group(1)  # jsonæ•°æ®
    json_to_dict(json_data)
    dict_data = json_to_dict(json_data)

    dict_list = []
    for item in dict_data:
        dit = {
            'ranking': item['ranking'],
            'password': item['passwd'],
            'cracking_time': item['time_to_crack_it'],
            'use_num': item['used_count'],
        }
        dict_list.append(dit)
    return dict_list


def save_data(datas, file_name='h04-å…¨çƒæœ€å¸¸ç”¨å¯†ç åˆ—è¡¨.csv'):
    """æ•°æ®ä¿å­˜"""
    save_directory = 'datas'
    os.makedirs(save_directory, exist_ok=True)  # exist_ok=True å¦‚æœç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™ä¸å¼•å‘é”™è¯¯, Falseåˆ™åä¹‹

    fieldnames = ['ranking', 'password', 'cracking_time', 'use_num']
    path = os.path.join(save_directory, file_name)  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    file_exists = os.path.exists(path)
    with open(path, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerows(datas)


def main():
    js_url = 'http://spiderbuf.cn/static/js/h04/udSL29.js'
    js_data = get_page(js_url)
    if js_data:
        dict_data = parse_data(js_data)
        save_data(dict_data)

    # decoded_data = eval(json_data)
    # print(decoded_data)
    # for item in decoded_data:
    #     print(item)


if __name__ == '__main__':
    main()
