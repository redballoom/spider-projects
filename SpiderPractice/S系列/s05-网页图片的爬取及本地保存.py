# -*- coding:utf-8 -*-
"""
âœï¸Author     : ğŸˆ
â²ï¸Time       : 2024/5/23
ğŸ“„File       : s05-ç½‘é¡µå›¾ç‰‡çš„çˆ¬å–åŠæœ¬åœ°ä¿å­˜.py
â„¹ï¸Description: ç®€çŸ­æè¿°è¯¥æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½æˆ–ç›®çš„

ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/s05/
"""
# å¯¼å…¥éœ€è¦çš„åº“
from urllib.parse import urljoin
import os.path
import requests
from lxml import etree


def get_page(url, headers=None):
    """
    é€šç”¨getè¯·æ±‚é¡µé¢ï¼Œç›´æ¥è¿”å›å¯¹è±¡æ–¹ä¾¿å‡½æ•°å¤ç”¨
    :param url: è¯·æ±‚é“¾æ¥
    :param headers: è¯·æ±‚å¤´
    :return: response å“åº”å¯¹è±¡
    """
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=default_headers, timeout=10)
        response.raise_for_status()  # å¦‚æœä¸æ˜¯2XXçŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
        return response
    except requests.exceptions.RequestException as e:  # é€šç”¨requestså¼‚å¸¸æ•è·
        print(f"Error making request to {url}: {e}")
        return None


def lxml_parse(html):
    """
    ä½¿ç”¨ lxml çš„ etree è§£æ HTML
    :param html: ç½‘é¡µæºä»£ç æ•°æ®
    :return: åŒ…è£…img_urlçš„ç”Ÿæˆå™¨
    """
    dom_tree = etree.HTML(html)
    img_url_list = dom_tree.xpath('//div[@class="table-responsive"]/div/img/@src')  # å¦‚æœä¸ç†Ÿæ‚‰xpathè¯­æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨æ§åˆ¶å°å…ƒç´ ä½ç½®copy
    for img_link in img_url_list:
        img_link = urljoin('http://www.spiderbuf.cn/', img_link)
        yield img_link


def save_data(file_name, data):
    """å›¾ç‰‡æ•°æ®ä¿å­˜"""
    save_directory = 'datas/s05å›¾ç‰‡ä¿å­˜'
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)
    path = os.path.join(save_directory, f'{file_name}.jpg')  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    # ä¿å­˜å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰äºŒè¿›åˆ¶æ•°æ®æ—¶ mode='wb'ï¼Œè¿™æ˜¯å›ºå®šçš„ï¼Œè¡¨ç¤ºäºŒè¿›åˆ¶å†™å…¥æ¨¡å¼ã€‚åŒæ—¶ä¹Ÿä¸èƒ½è¿›è¡Œencodingç¼–ç ã€‚
    with open(path, mode='wb') as f:
        f.write(data.content)  # ä¿å­˜äºŒè¿›åˆ¶æ•°æ®è¦ä½¿ç”¨contentå…¶ç±»å‹æ˜¯bytesï¼Œæ³¨æ„å®ƒæ˜¯å±æ€§çš„è°ƒç”¨


if __name__ == '__main__':
    page_url = 'http://www.spiderbuf.cn/s05/'
    html_data = get_page(page_url).text
    if html_data:
        gen_img = lxml_parse(html_data)
        for img_name, img_url in enumerate(gen_img):
            img_content = get_page(img_url)
            save_data(img_name, img_content)
