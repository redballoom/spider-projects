# -*- coding:utf-8 -*-
"""
âœï¸Author     : ğŸˆ
â²ï¸Time       : 2024/5/17
ğŸ“„File       : async_demo.py
â„¹ï¸Description: å¼‚æ­¥ç‰ˆæœ¬ï¼Œåªåšè¡¥å……å­¦ä¹ ç”¨

- å®é™…è¿è¡Œåªèƒ½è·å–5é¡µï¼Œå› ä¸ºä¼šæŠ¥429çš„é”™è¯¯ï¼Œéœ€è¦è®¾ç½®ä»£ç†ã€‚
- æˆ‘æ‰¾ä¸åˆ°å…è´¹çš„å¯ç”¨çš„ä»£ç†ï¼Œåˆ°æ­¤ä¾¿ä¸äº†äº†ä¹‹ã€‚
- ä»£ç æ³¨é‡Šé‡‡ç”¨çš„æ˜¯vscodeçš„æ’ä»¶ï¼ˆBaidu Comateï¼‰ã€‚
"""
import asyncio
import csv
import re
from urllib.parse import urljoin

from lxml import etree
import aiohttp
import aiofiles
from fake_useragent import UserAgent

base_url = 'https://www.52pojie.cn/'
fieldnames = ['album_title', 'album_url', 'album_rss', 'album_review', 'album_author', 'album_latest_time',
              'album_latest_topic']
save_file_name = 'async_demo.csv'


async def get_page(session, url):
    # åˆ›å»ºä¸€ä¸ª UserAgent å¯¹è±¡
    ua = UserAgent()

    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        # æ¥å—çš„å“åº”å†…å®¹ç±»å‹
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        # è¿æ¥ç±»å‹ï¼Œä¿æŒè¿æ¥
        "Connection": "keep-alive",
        # éšæœºé€‰æ‹©ä¸€ä¸ª User-Agent
        "User-Agent": ua.random,
    }

    try:
        # å‘èµ· GET è¯·æ±‚
        async with session.get(url, headers=headers, timeout=10) as response:
            # ç­‰å¾… 1 ç§’
            await asyncio.sleep(1)
            # æ£€æŸ¥å“åº”çŠ¶æ€ç ï¼Œå¦‚æœå¼‚å¸¸åˆ™æŠ›å‡ºå¼‚å¸¸
            response.raise_for_status()
            # è¿”å›å“åº”çš„æ–‡æœ¬å†…å®¹
            return await response.text()
    except aiohttp.ClientError as err:
        # æ‰“å° GET è¯·æ±‚é”™è¯¯
        print(f"Error making GET request to {url}: {err}")
        # è¿”å› None
        return None


async def get_index_page(session, page):
    # æ„é€ é¡µé¢URL
    page_url = f'https://www.52pojie.cn/forum.php?mod=collection&order=dateline&op=all&page={page}'
    # å¼‚æ­¥è°ƒç”¨get_pageå‡½æ•°è·å–é¡µé¢HTMLæ•°æ®
    html_data = await get_page(session, page_url)
    if html_data:
        # æ‰“å°å½“å‰è§£æçš„é¡µç 
        print(f'......å¼€å§‹è§£æç¬¬{page}é¡µ......')
        # è°ƒç”¨parseå‡½æ•°è§£æHTMLæ•°æ®å¹¶è¿”å›ç»“æœ
        return parse(html_data)
    else:
        # å¦‚æœæ²¡æœ‰è·å–åˆ°HTMLæ•°æ®ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨
        return []


def parse(html):
    # è§£æhtmlå­—ç¬¦ä¸²ï¼Œç”ŸæˆHTMLè§£ææ ‘
    tree = etree.HTML(html)
    # æŸ¥æ‰¾æ‰€æœ‰classåŒ…å«"clct_list"çš„divå…ƒç´ 
    divs = tree.xpath('//div[contains(@class, "clct_list")]/div')
    # ç”¨äºå­˜å‚¨è§£æç»“æœçš„åˆ—è¡¨
    data_list = []

    # éå†æ¯ä¸ªdivå…ƒç´ 
    for item in divs:
        # æå–ä¸“è¾‘æ ‡é¢˜
        album_title = item.xpath('./dl/dt/div/a/text()')[0]
        # æå–ä¸“è¾‘é“¾æ¥çš„hrefå±æ€§å€¼
        album_href = item.xpath('./dl/dt/div/a/@href')[0]
        # æ‹¼æ¥å®Œæ•´çš„ä¸“è¾‘é“¾æ¥
        album_url = urljoin(base_url, album_href)
        # æå–ä¸“è¾‘ä¿¡æ¯ä¸­çš„æ–‡æœ¬å†…å®¹
        album_info = item.xpath('./dl/dd[2]/p[2]/text()')[0]
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¸“è¾‘è®¢é˜…æ•°
        album_rss = re.search(r'è®¢é˜… (\d+)', album_info).group(1) if re.search(r'è®¢é˜… (\d+)', album_info) else ''
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¸“è¾‘è¯„è®ºæ•°
        album_review = re.search(r'è¯„è®º (\d+)', album_info).group(1) if re.search(r'è¯„è®º (\d+)', album_info) else ''
        # æå–ä¸“è¾‘ä½œè€…
        album_author = item.xpath('./dl/dd[2]/p[3]/a/text()')[0]
        # æå–ä¸“è¾‘æ›´æ–°æ—¶é—´
        album_update_time = item.xpath('./dl/dd[2]/p[3]/text()')[0]
        # æ›¿æ¢å­—ç¬¦ä¸²ä¸­çš„"åˆ›å»º, æœ€åæ›´æ–°"ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå¾—åˆ°ä¸“è¾‘æœ€æ–°æ—¶é—´
        album_latest_time = re.sub('åˆ›å»º, æœ€åæ›´æ–°', '', album_update_time)
        # æå–ä¸“è¾‘æœ€æ–°ä¸»é¢˜
        album_latest_topic = item.xpath('./dl/dd[2]/p[4]/a/text()')[0] if item.xpath('./dl/dd[2]/p[4]/a/text()') else ''

        # å°†è§£æçš„ä¸“è¾‘ä¿¡æ¯æ·»åŠ åˆ°data_liståˆ—è¡¨ä¸­
        data_list.append({
            'album_title': album_title,  # ä¸“è¾‘æ ‡é¢˜
            'album_url': album_url,  # ä¸“è¾‘é“¾æ¥
            'album_rss': album_rss,  # ä¸“è¾‘è®¢é˜…
            'album_review': album_review,  # ä¸“è¾‘è¯„è®º
            'album_author': album_author,  # ä¸“è¾‘ä½œè€…
            'album_latest_time': album_latest_time,  # ä¸“è¾‘æœ€æ–°æ—¶é—´
            'album_latest_topic': album_latest_topic  # ä¸“è¾‘æœ€æ–°ä¸»é¢˜
        })

    return data_list


async def save_to_csv(lock, data):
    # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›
    if not data:
        return

    # å¼‚æ­¥è·å–é”
    async with lock:
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶æŒ‡å®šæ¨¡å¼ä¸ºè¿½åŠ ã€æ— æ¢è¡Œç¬¦ã€ç¼–ç ä¸ºutf-8
        async with aiofiles.open('save_file_name', mode='a', newline='', encoding='utf-8') as f:
            # åˆ›å»ºcsv.DictWriterå¯¹è±¡ï¼ŒæŒ‡å®šå†™å…¥æ–‡ä»¶çš„å­—æ®µå
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            # éå†æ•°æ®åˆ—è¡¨
            for row in data:
                # å°†æ¯è¡Œæ•°æ®çš„å€¼ä»¥é€—å·åˆ†éš”ï¼Œå¹¶æ·»åŠ æ¢è¡Œç¬¦ï¼Œç„¶åå†™å…¥æ–‡ä»¶
                await f.write(','.join(row.values()) + '\n')


async def main():
    # åˆ›å»ºä¸€ä¸ªå¼‚æ­¥é”
    lock = asyncio.Lock()

    # åˆ›å»ºä¸€ä¸ªå¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¼šè¯
    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºä¸€ç»„å¼‚æ­¥ä»»åŠ¡ï¼Œç”¨äºè·å–ç´¢å¼•é¡µé¢
        tasks = [get_index_page(session, i) for i in range(1, 11)]  # 10 ç»„data_listæ•°æ®
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå¹¶è·å–é¡µé¢æ•°æ®
        pages_data = await asyncio.gather(*tasks)
        # æå–éç©ºé¡µé¢æ•°æ®ï¼Œå¹¶æ‹¼æ¥æˆæ€»æ•°æ®åˆ—è¡¨
        total_data = [data for page_data in pages_data for data in page_data if page_data]

        # åˆå§‹åŒ–CSVï¼Œå†™å…¥å¤´éƒ¨
        async with aiofiles.open('save_file_name', mode='w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            await writer.writeheader()

        # ä½¿ç”¨é”å¹¶å‘åœ°å†™å…¥æ•°æ®
        await asyncio.gather(*(save_to_csv(lock, [data]) for data in total_data))

        print('ä¿å­˜å®Œæ¯•ï¼')


if __name__ == '__main__':
    asyncio.run(main())


