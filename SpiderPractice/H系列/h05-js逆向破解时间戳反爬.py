# -*- coding:utf-8 -*-
"""
âœï¸Author     : ğŸˆ
â²ï¸Time       : 2024/5/27
ğŸ“„File       : h05-jsé€†å‘ç ´è§£æ—¶é—´æˆ³åçˆ¬.py
â„¹ï¸Description: ç®€çŸ­æè¿°è¯¥æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½æˆ–ç›®çš„

http://www.spiderbuf.cn/h05/api/MTcxNjkxMTA1MSw0NDk2ZTgxY2EwNGRkYjYyNjU0NDg0OTkwMWRkNDcyMg==
http://www.spiderbuf.cn/h05/api/MTcxNjkxMTA3OCxjNjE4OTg0ZTY2NGM4NDAxYWFlYzFjMTAwOTk3ZWU2Zg==

var timeStamp = Math[_0xf48905(0x80)](new Date()[_0xf48905(0x71)]() / 0x3e8)
  , _md5 = md5(timeStamp)
  , s = btoa(timeStamp + ',' + _md5);
sçš„å€¼å°±æ˜¯apiåé¢çš„è·¯å¾„ï¼Œåªéœ€è¦åœ¨pythonä¸­å®ç°è¯¥æ–¹æ³•å³å¯

æ¯æ¬¡åˆ·æ–°éƒ½ä¼šæ›´æ–°é“¾æ¥
"""
import os
import time
import csv
import hashlib
import base64
from urllib.parse import urljoin

import requests


def get_page(url, headers=None, retries=3):
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    }

    for attempt in range(retries):
        try:
            response = requests.get(url, headers=default_headers, timeout=10)
            response.raise_for_status()  # å¦‚æœä¸æ˜¯200çŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
            return response.json()
        except requests.exceptions.RequestException as e:  # é€šç”¨å¼‚å¸¸æ•è·
            print(f"Error making request to {url}: {e}. Attempt {attempt + 1} of {retries}.")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿ç­–ç•¥
    return None


def get_url_path():
    # è·å–å½“å‰æ—¶é—´æˆ³å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    timestamp = str(int(time.time()))
    # è®¡ç®—æ—¶é—´æˆ³çš„MD5å“ˆå¸Œå€¼
    timestamp_md5 = hashlib.md5(timestamp.encode('utf-8')).hexdigest()
    # å°†æ—¶é—´æˆ³å’ŒMD5å“ˆå¸Œå€¼ä»¥é€—å·åˆ†éš”å¹¶ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²
    s_result = base64.b64encode(f'{timestamp},{timestamp_md5}'.encode('utf-8')).decode('utf-8')
    print(s_result)
    return s_result


def parse_data(data_list):
    dict_list = []
    for item in data_list:
        dit = {
            'ranking': item['ranking'],
            'password': item['passwd'],
            'cracking_time': item['time_to_crack_it'],
            'use_num': item['used_count'],
        }
        dict_list.append(dit)
    return dict_list


def save_data(datas, file_name='h05-å…¨çƒæœ€å¸¸ç”¨å¯†ç åˆ—è¡¨.csv'):
    """æ•°æ®ä¿å­˜"""
    save_directory = 'datas'
    os.makedirs(save_directory, exist_ok=True)  # exist_ok=True å¦‚æœç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™ä¸å¼•å‘é”™è¯¯, Falseåˆ™åä¹‹

    fieldnames = ['ranking', 'password', 'cracking_time', 'use_num']
    path = os.path.join(save_directory, file_name)  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    file_exists = os.path.exists(path)
    with open(path, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerows(datas)


def main():
    base_url = 'http://www.spiderbuf.cn/h05/'
    s = get_url_path()
    api_url = urljoin(base_url, f'api/{s}')

    json_data = get_page(api_url)
    if json_data:
        dict_data = parse_data(json_data)
        save_data(dict_data)


if __name__ == '__main__':
    main()
