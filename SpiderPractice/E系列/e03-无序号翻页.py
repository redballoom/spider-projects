# _*_ coding: utf-8 _*_
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2023å¹´12æœˆ30
@ ğŸ“„File       : e03-æ— åºå·ç¿»é¡µ.py
@ â„¹ï¸Description:
ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/e03/

http://www.spiderbuf.cn/e03/2fe6286a4e5f  1
http://www.spiderbuf.cn/e03/5f685274073b  2
æ§åˆ¶ç¿»é¡µçš„ä¸åœ¨æ˜¯æ™®é€šæ•°å­—ï¼Œè€Œæ˜¯åœ¨å‰ç«¯ä»£ç ä¸­é¡µç ä½ç½®ï¼Œåªè¦è·å–ä¸‹æ¥æ‹¼æ¥åˆ°é“¾æ¥åå³å¯
"""

# å¯¼å…¥éœ€è¦çš„åº“
import os.path
from urllib.parse import urljoin

import requests
from lxml import etree

base_url = 'http://www.spiderbuf.cn/e03/'
FOLDER_NAME = 'datas'
if not os.path.exists(FOLDER_NAME):
    os.mkdir(FOLDER_NAME)


def get_page(url, headers=None, retries=3):  # retries: è‡ªå®šä¹‰é‡è¯•3æ¬¡
    """é€šç”¨è¯·æ±‚æ–¹æ³•"""
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    }

    for _ in range(retries):
        try:
            response = requests.get(url, headers=default_headers, timeout=10)
            response.raise_for_status()  # å¦‚æœä¸æ˜¯200çŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
            return response.text
        except requests.exceptions.RequestException as e:  # é€šç”¨å¼‚å¸¸æ•è·
            print(f"Error making request to {url}: {e}")
    return None


def get_index_page_url(url):
    """
    é€šè¿‡è·å–é¡µç çš„è·¯å¾„æ¥æ‹¼æ¥å‡ºå®Œæ•´çš„è¯·æ±‚é“¾æ¥ï¼Œè¿”å›page_urlçš„ç”Ÿæˆå™¨
    :param url: base_urlçš„é“¾æ¥
    :return: page_url
    """
    html_data = get_page(url)
    # æ·»åŠ åˆ¤æ–­ï¼Œåœ¨é‡è¯•ä¸‰æ¬¡åhtml_dataçš„å€¼ä¸ºNone åˆ™ç›´æ¥é€€å‡ºç¨‹åº
    if not html_data:
        return
    dom_tree = etree.HTML(html_data)
    page_path_list = dom_tree.xpath('//ul[@class="pagination"]/li/a/@href')
    for page_path in page_path_list:
        page_url = urljoin(url, page_path)  # é€šè¿‡urljoinæ¥æ‹¼æ¥è·¯å¾„
        print(f'é‡‡é›†å½“å‰é¡µæ•°æ®ï¼š{page_url}')
        yield page_url


def lxml_parse(html):
    """
    ä½¿ç”¨ lxml çš„ etree è§£æ HTML
    :param html: ä¸€é¡µçš„ç½‘é¡µæºä»£ç æ•°æ®
    :return: è§£æåçš„ä¸€é¡µæ•°æ®åˆ—è¡¨
    """
    dom_tree = etree.HTML(html)
    trs = dom_tree.xpath('//table[@class="table"]/tbody/tr')
    page_data = []
    for tr in trs:
        ranking = tr.xpath('./td[1]/text()')[0]
        value = tr.xpath('./td[2]/text()')[0]
        enterprise_information = tr.xpath('./td[3]/text()')[0]
        ceo_name = tr.xpath('./td[4]/text()')[0]
        profession = tr.xpath('./td[5]/text()')[0]
        row_data = ','.join([ranking, value, enterprise_information, ceo_name, profession, '\n'])
        page_data.append(row_data)
    return page_data


def save_data(file_data, file_name='e03èƒ¡æ¶¦ä¸­å›½500å¼º.txt'):
    """æ•°æ®ä¿å­˜"""
    path = os.path.join(FOLDER_NAME, file_name)  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    with open(path, mode='a', encoding='utf-8') as f:
        f.writelines(file_data)


def main():
    gen_page_url = get_index_page_url(base_url)
    for page_url in gen_page_url:
        page_html = get_page(page_url)
        if page_html:
            page_data = lxml_parse(page_html)
            save_data(page_data)
        else:
            print(f"Failed to retrieve or parse page: {page_url}")


if __name__ == '__main__':
    main()
