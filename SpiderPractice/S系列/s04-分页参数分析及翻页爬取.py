# _*_ coding: utf-8 _*_
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2023å¹´12æœˆ30
@ ğŸ“„File       : s04-åˆ†é¡µå‚æ•°åˆ†æåŠç¿»é¡µçˆ¬å–.py
@ â„¹ï¸Description:
ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/s04/

åˆ†é¡µè§„å¾‹ï¼š
    http://www.spiderbuf.cn/s04/?pageno=1
    http://www.spiderbuf.cn/s04/?pageno=2

åˆ†é¡µå‚æ•°ï¼špageno
"""
# å¯¼å…¥éœ€è¦çš„åº“
from urllib.parse import urljoin
import os.path

import requests
from lxml import etree


def get_page(url, headers=None, **kwargs):
    """
    é€šç”¨getè¯·æ±‚è·å–é¡µé¢æºç 
    :param url: è¯·æ±‚é“¾æ¥
    :param headers: è¯·æ±‚å¤´
    :return: å“åº”å¯¹è±¡response
    """
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=default_headers, timeout=10, **kwargs)
        response.raise_for_status()  # å¦‚æœä¸æ˜¯2xxçŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
        return response.text
    except requests.exceptions.RequestException as e:  # é€šç”¨requestså¼‚å¸¸æ•è·
        print(f"Error making request to {url}: {e}")
        return None


def get_index_page(total_page):
    """è·å–å¤šé¡µurlé“¾æ¥ï¼Œå¹¶è¿”å›urlçš„ç”Ÿæˆå™¨"""
    for page in range(1, total_page + 1):
        # æ„å»ºç‰¹å®šé¡µç çš„ URL
        url = urljoin(base_url, f'/s04/?pageno={page}')
        yield url


def lxml_parse(html):
    """
    ä½¿ç”¨ lxml çš„ etree è§£æ HTML
    :param html:
    :return: page_data_list: ä¸€é¡µçš„æ•°æ®åˆ—è¡¨
    """
    dom_tree = etree.HTML(html)
    trs = dom_tree.xpath('/html/body/div/div[1]/table/tbody/tr')  # å¦‚æœä¸ç†Ÿæ‚‰xpathè¯­æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨æ§åˆ¶å°å…ƒç´ ä½ç½®copy
    # é€šç”¨æ–¹æ³•ï¼šå…ˆå®šä½åˆ°ä¸€ä¸ªä¸ªâ€œç›’å­â€ï¼Œå†ä»â€œç›’å­â€ä¸­éå†å–æ•°æ®ã€‚
    page_data_list = []  # å­˜æ”¾ä¸€é¡µçš„æ•°æ®
    for tr in trs:
        index = tr.xpath('./td[1]/text()')[0]
        ip_addr = tr.xpath('./td[2]/text()')[0]
        mac_addr = tr.xpath('./td[3]/text()')[0]
        device_name = tr.xpath('./td[4]/text()')[0]
        device_type = tr.xpath('./td[5]/text()')[0]
        system_name = tr.xpath('./td[6]/text()')[0]
        open_port = tr.xpath('./td[7]/text()')[0] if tr.xpath('./td[7]/text()') else 'None'
        status = tr.xpath('./td[8]/text()')[0]
        file_data = f'{index}|{ip_addr}|{mac_addr}|{device_name}|{device_type}|{system_name}|{open_port}|{status}\n'
        page_data_list.append(file_data)
    return page_data_list


def save_data(data_list, file_name='s04-è®¾å¤‡ä¿¡æ¯.txt'):
    """æ•°æ®ä¿å­˜"""
    folder_name = 'datas'
    if not os.path.exists(folder_name):
        os.mkdir(folder_name)
    path = os.path.join(folder_name, file_name)  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    with open(path, mode='a', encoding='utf-8') as f:
        f.writelines(data_list)


if __name__ == '__main__':
    # æ­¤å†…ç›¸å½“äºå…¨å±€ç¯å¢ƒ
    PAGE = 5  # è¦è·å–çš„é¡µç æ•°é‡
    base_url = 'http://www.spiderbuf.cn/'
    gen_url = get_index_page(PAGE)  # è·å–å¤šé¡µçš„urlç”Ÿæˆå™¨
    # æˆ‘ä»¬åœ¨å¤„ç†å¤§é‡æ•°æ®å­˜å‚¨æ—¶åº”é¿å…ä¸€ä¸ªä¸€ä¸ªæ·»åŠ ï¼Œè¿™æ ·æ•ˆç‡ä½ï¼Œå¼€é”€å¤§ï¼Œåœ¨å¤„ç†å®Œæ•°æ®ç›´æ¥ä¸€æ¬¡æ€§å­˜å‚¨ä¼šæ›´å¥½ã€‚ä½†æ˜¯
    # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œä¸€æ¬¡æ€§å­˜å‚¨å¯èƒ½ä¼šå¯¼è‡´å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥é‡‡ç”¨åˆ†æ‰¹æ¬¡å¤„ç†çš„æ–¹æ³•ã€‚
    # æˆ‘ä¸€èˆ¬éƒ½é€‰æ‹©åˆ†æ‰¹æ¬¡å¤„ç†ï¼ˆæŒ‰é¡µå¤„ç†ï¼‰ï¼Œè¿™æ ·åœ¨ä¸­æ–­ç¨‹åºæ—¶ä¹Ÿèƒ½ä¿è¯ä¸€éƒ¨åˆ†æ•°æ®çš„å­˜å‚¨ã€‚
    # æ€»ä¹‹ï¼Œåœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µæ¥é€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚ä¸€æ¬¡æ€§å­˜å‚¨å’Œåˆ†æ‰¹æ¬¡å¤„ç†éƒ½å¯èƒ½æ˜¯æœ‰ç”¨çš„é€‰æ‹©ã€‚
    for page_url in gen_url:
        print(f'å½“å‰å¤„ç†ç½‘é¡µï¼š{page_url}')
        # è°ƒç”¨ä½ çš„çˆ¬å–å‡½æ•°æ¥å¤„ç†è¿™ä¸ªç‰¹å®šé¡µç çš„æ•°æ®
        html_data = get_page(page_url)
        # è°ƒç”¨è§£æå‡½æ•°
        one_page_data = lxml_parse(html_data)
        save_data(one_page_data)
