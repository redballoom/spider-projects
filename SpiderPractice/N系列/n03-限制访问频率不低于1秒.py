# _*_ coding: utf-8 _*_
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2023å¹´12æœˆ30
@ ğŸ“„File       : n03-é™åˆ¶è®¿é—®é¢‘ç‡ä¸ä½äº1ç§’.py
@ â„¹ï¸Description:
ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/n03/

å¯¹äºé™åˆ¶äº†è¯·æ±‚è®¿é—®é¡µé¢æ—¶é—´çš„ç½‘ç«™ï¼Œåªéœ€è¦åœ¨è¯·æ±‚åæ·»åŠ time.sleep(N)å³å¯ï¼Œå¯è¿™æ ·å°±å˜å¾—å¾ˆæ…¢ï¼Œå¤§å¤§å½±å“æˆ‘ä»¬çš„å¿ƒæƒ…ï¼Œ
è¿™æ—¶å°±å¯ä»¥ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹æ¥å¹¶è¡Œè¯·æ±‚ç½‘é¡µæ•°æ®ï¼Œå‰ææ˜¯ä½ å¾—å‡†å¤‡å¥½å¯ç”¨çš„ipæ± 
"""
import csv
import time
import os.path
from urllib.parse import urljoin

import requests
from lxml import etree


def get_page(url, headers=None, retries=3):
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    }

    for attempt in range(retries):
        try:
            response = requests.get(url, headers=default_headers, timeout=10)
            response.raise_for_status()  # å¦‚æœä¸æ˜¯200çŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
            return response.text
        except requests.exceptions.RequestException as e:  # é€šç”¨å¼‚å¸¸æ•è·
            print(f"Error making request to {url}: {e}. Attempt {attempt + 1} of {retries}.")
            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…ä¸€æ®µæ—¶é—´
    return None


def get_index_page(url, pages):
    """è·å–å¤šé¡µæ•°æ®"""
    for page in range(1, pages + 1):
        # æ„å»ºç‰¹å®šé¡µç çš„ URL,ä¸å»ºè®®å­—ç¬¦ä¸²æ‹¼æ¥çš„æ–¹å¼æ¥æ„é€ urlé“¾æ¥ï¼Œå¯ä»¥ä½¿ç”¨urllib.parseçš„urljoin
        page_url = urljoin(url, f'/n03/{page}')
        # è°ƒç”¨ä½ çš„çˆ¬å–å‡½æ•°æ¥å¤„ç†è¿™ä¸ªç‰¹å®šé¡µç çš„æ•°æ®
        html_data = get_page(page_url)
        time.sleep(1)

        if html_data:
            print(f"Be dealing withï¼š{page_url} ...")
            lxml_parse(html_data)
        else:
            print(f"Failed to retrieve data from {page_url}")


def lxml_parse(html):
    """
    ä½¿ç”¨ lxml çš„ etree è§£æ HTML
    :param html: ç½‘é¡µçš„æºä»£ç æ•°æ®
    :return: None
    """
    dom_tree = etree.HTML(html)
    trs = dom_tree.xpath('//table[@class="table"]/tbody/tr')
    page_data = []
    for tr in trs:
        ranking = tr.xpath('./td[1]/text()')[0] if tr.xpath('./td[1]/text()') else ''
        pw = tr.xpath('./td[2]/text()')[0] if tr.xpath('./td[2]/text()') else ''
        cracking_time = tr.xpath('./td[3]/text()')[0].replace('< ', '') if tr.xpath('./td[3]/text()') else ''
        use_num = tr.xpath('./td[4]/text()')[0] if tr.xpath('./td[4]/text()') else ''

        dit = {
            'ranking': ranking,
            'password': pw,
            'cracking_time': cracking_time,
            'use_num': use_num,
        }
        page_data.append(dit)
    save_data(page_data)


def save_data(datas, file_name='n03-å…¨çƒæœ€å¸¸ç”¨å¯†ç åˆ—è¡¨.csv'):
    """æ•°æ®ä¿å­˜"""
    save_directory = 'datas'
    os.makedirs(save_directory, exist_ok=True)  # exist_ok=True å¦‚æœç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™ä¸å¼•å‘é”™è¯¯, Falseåˆ™åä¹‹

    fieldnames = ['ranking', 'password', 'cracking_time', 'use_num']
    path = os.path.join(save_directory, file_name)  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    file_exists = os.path.exists(path)
    with open(path, mode='a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerows(datas)


def main():
    total_page = 5  # è¦è·å–çš„é¡µç æ•°é‡
    base_url = f'http://www.spiderbuf.cn/'
    get_index_page(base_url, total_page)


if __name__ == '__main__':
    main()
