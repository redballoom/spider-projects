# -*- coding:utf-8 -*-
"""
âœï¸Author     : ğŸˆ
â²ï¸Time       : 2024/5/23
ğŸ“„File       : s07-jaxåŠ¨æ€åŠ è½½æ•°æ®çš„çˆ¬å–.py
â„¹ï¸Description: ç®€çŸ­æè¿°è¯¥æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½æˆ–ç›®çš„

ç»ƒä¹ ç›®æ ‡ï¼šhttp://www.spiderbuf.cn/s07/

æµç¨‹ï¼š
    - 1.æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œç‚¹å‡»ç½‘ç»œé€‰é¡¹å¡ï¼Œç¡®ä¿æ‰“å¼€è¿‡æ»¤å™¨ï¼Œç„¶åç‚¹å‡»XHRæŒ‰é’®
    - 2.ç‚¹å‡»åˆ·æ–°ç½‘é¡µï¼ŒæŸ¥çœ‹åŠ è½½çš„æ•°æ®æ¥å£ï¼Œåœ¨é¢„è§ˆä¸­å¦‚æœæœ‰æ•°æ®å°±æ˜¯æˆ‘ä»¬è¦çš„æ¥å£ã€‚

ç‰¹æ®Šçš„è¯·æ±‚å¤´ï¼Œè¿™å°±æ˜¯ajaxè¯·æ±‚ï¼Œè¿™ä¸ªè¯·æ±‚å¤´é€šå¸¸è¢«ç”¨äºæ ‡è¯†ä¸€ä¸ªè¯·æ±‚æ˜¯ä¸€ä¸ªAJAXè¯·æ±‚ã€‚
    X-Requested-With: XMLHttpRequest
"""
# å¯¼å…¥éœ€è¦çš„åº“
import json
import os.path

import requests


def get_page(url, headers=None):
    """
    getè¯·æ±‚é¡µé¢
    :param url: è¯·æ±‚é“¾æ¥
    :param headers: è¯·æ±‚å¤´
    :return: ç½‘é¡µæºä»£ç æ•°æ®
    """
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=default_headers, timeout=10)
        response.encoding = 'utf-8'  # å¦‚æœå‡ºç°ä¹±ç å°±å»ç½‘é¡µæºä»£ç ä¸­æ‰¾charsetï¼Œå®ƒæ˜¯ä»€ä¹ˆè¿™é‡Œçš„ç¼–ç å°±æ˜¯ä»€ä¹ˆã€‚
        response.raise_for_status()  # å¦‚æœä¸æ˜¯2XXçŠ¶æ€ç è¯·æ±‚ï¼Œåˆ™æŠ›å‡ºHTTPErroré”™è¯¯
        # å¦‚æœç›®æ ‡ç½‘ç«™æ˜¯ajaxæ¥å£å°±å¯ä»¥ç›´æ¥é€šè¿‡json()ï¼Œè¿”å›è§£æåçš„pythonå­—å…¸æ•°æ®
        return response.json()
    except requests.exceptions.RequestException as e:  # é€šç”¨requestså¼‚å¸¸æ•è·
        print(f"Error making request to {url}: {e}")
        return None


def parse(dit_data):
    """
    ä»å­—å…¸æ•°æ®ä¸­æå–æ•°æ®
    :param dit_data: æ•°æ®æ¥å£è¿”å›çš„æ•°æ®
    :return: page_data_list: æ•°æ®åˆ—è¡¨
    """
    # å­—å…¸æ•°æ®çš„å–å€¼ï¼Œæ¨èä½¿ç”¨get()æ–¹æ³•æ¥å–å€¼ï¼Œå› ä¸ºä½¿ç”¨å®ƒï¼Œåœ¨é”®ä¸å­˜åœ¨æ—¶ä¹Ÿä¸ä¼šæŠ¥é”™ã€‚
    page_data_list = []
    for data in dit_data:
        dit = {
            'ip': data.get('ip'),
            'mac': data.get('mac'),
            'manufacturer': data.get('manufacturer'),
            'name': data.get('name'),
            'ports': data.get('ports'),
            'status': data.get('status'),
            'type': data.get('type'),
        }
        page_data_list.append(dit)
    return page_data_list


def save_data(data, file_name='s07-è®¾å¤‡ä¿¡æ¯.json'):
    """æ•°æ®ä¿å­˜"""
    save_directory = 'datas'
    if not os.path.exists(save_directory):
        os.mkdir(save_directory)
    path = os.path.join(save_directory, file_name)  # å®Œæ•´æ–‡ä»¶è·¯å¾„
    with open(path, mode='w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


if __name__ == '__main__':
    page_url = 'http://www.spiderbuf.cn/iplist/?order=asc'
    # åœ¨çˆ¬è™«ä¸­ï¼Œå¦‚æœåœ¨è¯·æ±‚å¤´ä¸­çœ‹åˆ°ä»¥ä¸‹headersä¿¡æ¯ï¼Œå»ºè®®éƒ½æºå¸¦ä¸Š
    my_headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Content-Type': 'application/json',
        'Referer': 'http://www.spiderbuf.cn/s07/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    }
    json_data = get_page(page_url, headers=my_headers)
    if json_data:
        data_list = parse(json_data)
        save_data(data_list)
