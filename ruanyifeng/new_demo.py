# -*- coding:utf-8 -*-
"""
âœï¸Author     : ğŸˆ
â²ï¸Time       : 2024/5/21
ğŸ“„File       : new_demo.py
â„¹ï¸Description: ç®€çŸ­æè¿°è¯¥æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½æˆ–ç›®çš„

ç›®æ ‡åœ°å€: https://www.ruanyifeng.com/blog/weekly/
"""
import urllib3
from urllib3.util import Retry
from typing import Generator, List, Dict, Optional

from fake_useragent import UserAgent
import requests
from requests.adapters import HTTPAdapter
from loguru import logger
from pyquery import PyQuery

ua = UserAgent().random  # éšæœºUA
# ç¦æ­¢æ˜¾ç¤ºInsecureRequestWarningè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
session = requests.Session()


def get_request(url: str) -> Optional[str]:
    """
    é€šç”¨getè¯·æ±‚æ–¹æ³•
    :param url: ç›®æ ‡ç½‘ç«™çš„urlé“¾æ¥
    :return: ç›®æ ‡ç½‘ç«™çš„æºä»£ç ä¿¡æ¯
    """
    retries = Retry(total=3, backoff_factor=1)  # é‡è¯•3æ¬¡ï¼Œé‡è¯•é—´éš”1s
    session.mount('https://', HTTPAdapter(max_retries=retries))
    headers = {"User-Agent": ua}
    try:
        response = session.get(url, headers=headers, timeout=6, verify=False)
        response.encoding = response.apparent_encoding
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        logger.error(f"An error occurred during request: {e}")
        return None


def parse_page(html: str) -> Generator[str, None, None]:
    """
    è§£æé¡µé¢æ•°æ®ï¼Œè·å–è¯¦æƒ…é¡µurl
    :param html: ç½‘é¡µæºä»£ç æ•°æ®
    :return: Generator[href] åŒ…å«è¯¦æƒ…é¡µé“¾æ¥çš„ç”Ÿæˆå™¨
    """
    doc = PyQuery(html)  # åˆå§‹åŒ–PyQuery
    li_list = doc('#alpha-inner ul.module-list li').items()
    for li in li_list:
        href = li('a').attr('href')  # 300æ¡
        yield href


def get_detail_page(url: str) -> Optional[str]:
    """é€šè¿‡è·å–çš„è¯¦æƒ…é¡µurlï¼Œåœ¨å†…éƒ¨è°ƒç”¨get_requestæ–¹æ³•æ¥è·å–è¯¦æƒ…é¡µæ•°æ®"""
    logger.info(f"Start parsing the {url}...")
    return get_request(url)


def detail_parse_page(detail_html: str) -> List[Dict[str, str]]:
    """è§£æè¯¦æƒ…é¡µï¼Œæ‹¿åˆ°æœ€ç»ˆæ•°æ®"""
    doc = PyQuery(detail_html)
    resources = []
    try:
        # æ‰¾åˆ° <h2> èµ„æº
        resource_section = doc('h2:contains("èµ„æº")')
        # è·å–èµ„æºåçš„æ‰€æœ‰åŒçº§ <p> æ ‡ç­¾ç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªåŒçº§ <h2>
        current_item = {}

        for sibling in resource_section.next_all():
            if sibling.tag == 'h2':
                break
            if sibling.tag == 'p':
                p = PyQuery(sibling)
                if p('a').length > 0 and 'title' not in current_item:
                    current_item = {
                        'title': p('a').text(),
                        'href': p('a').attr('href'),
                        'desc': ''
                    }
                elif p('img').length > 0:
                    current_item['img'] = p('img').attr('src')
                elif p.text():
                    current_item['desc'] = p.text()
                    if 'title' in current_item:
                        resources.append(current_item)
                        current_item = {}
        return resources
    except Exception as e:
        logger.error(f"An error occurred while parsing the HTML content: {e}")
        return []


def main():
    page_url = "https://www.ruanyifeng.com/blog/weekly/"
    page_html = get_request(page_url)
    if not page_html:
        logger.error("Failed to retrieve the main page.")
        return
    generate_url = parse_page(page_html)
    for detail_url in generate_url:  # 300
        detail_page = get_detail_page(detail_url)
        if detail_page:
            resource_data = detail_parse_page(detail_page)  # ä¸€ä¸ªdetail_urlçš„æ•°æ®
            logger.info(resource_data)


if __name__ == '__main__':
    main()
