# -*- coding: utf-8 -*-
"""
@ ğŸ˜€Author     : ğŸˆ
@ â²ï¸Time       : 2024å¹´01æœˆ01
@ ğŸ“„File       : old.py
@ â„¹ï¸Description:

é‡‡é›† UPUPOO çš„å£çº¸

ä½¿ç”¨å·¥å…·:
  Progress Telerik Fiddler Classic
  pycharm

"""
import requests
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

FOLDER = 'wallpaper'
TAG_ID = 12
TOTAL_PAGE = 6  # æ€»é¡µæ•°

folder_path = Path(FOLDER)
# åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰parents: å…è®¸çˆ¶ç›®å½•ï¼Œexist_ok: å¦‚æœå­˜åœ¨ä¹Ÿä¸å‘ç”ŸæŠ¥é”™
Path.mkdir(folder_path, exist_ok=True)


def get_page(method, url, headers=None, **kwargs):
    """
    å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µå†…å®¹ã€‚

    Args:
        method (str): HTTPæ–¹æ³•ï¼Œå¦‚`GET`ã€`POST`ç­‰ã€‚
        url (str): è¦è¯·æ±‚çš„URLã€‚
        headers (dict, optional): ç”¨äºè¯·æ±‚çš„HTTPå¤´ã€‚é»˜è®¤ä¸ºNoneã€‚
        **kwargs: requestæ”¯æŒçš„æ‰€æœ‰å‚æ•°ã€‚

    Returns:
        requests.Response: HTTPå“åº”å¯¹è±¡ã€‚å¦‚æœå‡ºé”™ï¼Œè¿”å›Noneã€‚
    """
    default_headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) upupoo-wallpaper-shop/0.0.1 Chrome/80.0.3987.165 Electron/8.2.5 Safari/537.36',
    }

    try:
        resp = requests.request(method, url, headers=default_headers, timeout=10, **kwargs)
        resp.raise_for_status()
        return resp
    except requests.exceptions.RequestException as e:
        print(f"Error making {method} request to {url}: {e}")
        return None


def get_index_page(page):
    url = 'https://pcwallpaper.upupoo.com/wallpaper/lists'
    data = {"current": page, "size": 28, "total": 239311, "type": 0,
            "tagId": TAG_ID, "sort": 0, "resolution": 0, "hairtail": 0}
    return get_page('POST', url, json=data)


def parse(json_data):
    cards = json_data['data']['records']  # å­—å…¸æ•°æ®åˆ—è¡¨:
    for info in cards:
        paper_id = info['paper_id']
        img_url = info['img_url']

        # https://pcsource.upupoo.com/theme/2001206385/listCover.jpg
        # è¿™ä¸ªé“¾æ¥æ˜¯å°å›¾ï¼Œå¤§å›¾éœ€è¦æŠŠlistCoveræ¢ä¸ºpreviewFix
        new_filename = 'previewFix.jpg'
        parts = img_url.split('/')
        # æ›¿æ¢æ–‡ä»¶å
        parts[-1] = new_filename
        new_img_url = '/'.join(parts)
        yield paper_id, new_img_url


def download(data):
    # data æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ é‡Œé¢æ˜¯å…ƒç»„ (id, url)
    name, url = data  # ä½¿ç”¨è§£åŒ…æ˜¯å¾ˆå¥½çš„é€‰æ‹©ï¼Œå¦‚æœä½¿ç”¨å¾ªç¯ä¼šå¯¼è‡´å¤šè¿›ç¨‹ä¸èƒ½æˆåŠŸå·¥ä½œ
    response = get_page('GET', url)
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªåˆ¤æ–­æ˜¯å¾ˆå¥½çš„é€‰æ‹©ï¼Œä»¥å…é”™è¯¯çš„è¿”å›None,å¯¼è‡´ä½¿ç”¨response.contentæŠ¥é”™
    if response:
        img_content = response.content
        file_path = folder_path / f'{name}.jpg'  # ç¡®ä¿æ–‡ä»¶åçš„æ­£ç¡®æ€§
        with open(file_path, mode='wb') as f:
            f.write(img_content)
            print(f'Successful -> {url}')
    else:
        print('Response has no content.')


def start_thread_pool(func, iterable):
    # å»ºè®®ä¸è¦å°†max_workersè®¾ç½®å¤ªå¤§ï¼Œåº”é¿å…ç»™å¯¹æ–¹æœåŠ¡å™¨é€ æˆå‹åŠ›ï¼Œå°¤å…¶æ˜¯è¯·æ±‚é¡µæ•°å¤šçš„æ—¶å€™
    with ThreadPoolExecutor(max_workers=12)as executor:
        executor.map(func, iterable)


if __name__ == '__main__':
    for i in range(1, TOTAL_PAGE + 1):
        html = get_index_page(i)
        parse(html.json())
        g_data = parse(html.json())
        # å¯åŠ¨è¿›ç¨‹æ± 
        start_thread_pool(download, g_data)
